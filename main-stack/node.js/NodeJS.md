# Самообучающийся материал по Node.js

## Оглавление

- [Введение](#введение)
- [Зачем нужен Node.js](#зачем-нужен-nodejs)
- [Ключевые особенности Node.js](#ключевые-особенности-nodejs)
- [Модули в Node.js](#модули-в-nodejs)
  - [Какие модули бывают](#какие-модули-бывают)
    - [IIFE (Immediately Invoked Function Expression)](#IIFE)
    - [CommonJS - разница между `exports` и `module.exports`](#CommonJS)
    - [EcmaScript modules - виды `import` и `export`, и фазы поиска их](#ESM)
  - [Module core (модульная система)](#module-system)
    - [Способы взаимодействия с путями и глобальные переменные](#работа-с-url)
    - [process (содержит информацию о текущем запущенном процессе Node.js)](#module-system-process)
    - [file system (работа с файлами)](#module-system-file-system)
    - [os (информация об ОС)](#module-system-os)
    - [streams (работа с потоками данных)](#events-and-streams)
      - [Writable stream](#writable-stream);
      - [Readable stream](#readable-stream);
      - [Duplex stream](#dublex-stream);
      - [Transform stream](#transform-stream);
    - [zlib (сжатия данных)](#zlib)
    - [crypt (шифрования)](#crypto)
    - [http/https (создание сервера)](#module-system-http)
    - [timers & related](#module-system-timers-and-related)
- [Многопоточность в node-js](#многопоточность)
  - [Cluster](#cluster)
  - [Worker threads](#worker-threads)
  - [Child process (дочерний процесс) - CP](#childProcess);

---

<a id="введение"></a>
## Введение

Node.js - это среда выполнения JavaScript, построенная на движке V8, которая позволяет исполнять JS-код вне браузера, на сервере. С её помощью можно взаимодействовать с файловой системой, операционной системой, сетью и другими низкоуровневыми компонентами.

<a id="зачем-нужен-nodejs"></a>
## Зачем нужен Node.js

1. Когда мы хотим построить input, output - интенсивное приложение. Это означает, что есть задачи, которые не имеют так много вычислений, но при этом очень много данных идет на вход и много нужно отдавать на выход. Например - API (что-то отдают на фронт, получают с него запросы, делают несложные действия - например достать что-то из базы, что-то туда записать и отдать на фронт результат);

2. Когда мы хотим отдавать какие-то данные по частям (data streaming applications) - например мы можем отдавать видео, какие-то другие данные в потоке;

3. CLI (Command line interface) applications - это интерфейс командной строки. Если нам не нужен UI, и какая-то красивая графика и мы хотим чтобы наше приложение выполняло какие-то действия;

4. IoT (Internet of things) - устройство для умного дома, которое контролирует освещение, для контроля температуры в доме, включения лампочек и т.д. А в node-js он используется для применения в микроконтроллерах;

5. Cloud - если мы захотим использовать такие cloud провайдеры как Amazon Web Services или google cloud platform и microsoft azure, то с помощью него (node.js) мы можем писать лямбды функции, и мы можем писать приложения, которые там будем разворачивать;

<a id="ключевые-особенности-nodejs"></a>
## Какие есть ключевые особенность у Node.js

1. **Среда выполнения JavaScript**. Node.js позволяет выполнять JavaScript-код вне браузера, на сервере.

2. **Движок V8**. Node.js использует движок V8 от Google, который применяет JIT-компиляцию (Just-In-Time). Это позволяет очень быстро компилировать и выполнять код.

3. **Однопоточная модель с Event Loop**. В отличие от сред, где под каждый запрос создается отдельный поток (что потребляет много памяти и ресурсов), Node.js использует однопоточную модель. В её основе лежит Event Loop (цикл событий), который обрабатывает события и асинхронные операции. Крайне важно не блокировать его выполнением тяжелых синхронных операций.

4. **Кроссплатформенность**. Node.js позволяет писать код, который будет работать на разных операционных системах (Windows, Linux, macOS) и устройствах (компьютеры, серверы, иногда даже микроконтроллеры).

5. **Событийно-ориентированная архитектура (Event-Driven Architecture)**. Node.js построен на событиях, что идеально подходит для обработки множества одновременных операций ввода-вывода (например, запросов к базе данных, сетевых вызовов).

6. **Высокая масштабируемость (Scalability)**. Благодаря своей архитектуре Node.js очень хорошо масштабируется. Можно легко создавать новые экземпляры приложения с помощью дочерних процессов (child processes), выделять тяжелые задачи в отдельные потоки (worker threads) и масштабировать приложение с помощью кластеризации (cluster).

7. **Богатый встроенный API**. Node.js обладает обширной стандартной библиотекой, содержащей множество встроенных модулей (для работы с файловой системой, сетью, HTTP и т.д.). Это предоставляет готовый функционал для решения основных задач.

<a id="модули-в-nodejs"></a>
## Модули в Node.js

В node.js любой файл считает модулем. Модуль в свою очередь - это такой способ организации кода, когда отдельный какой-то функционал помещается в отдельный файл, и соответственно содержимое этого файла оно экспортируется / импортируется в другие файлы (модули).

### Основные преимущества использования модулей

1. Кодовая база делится на различные файлы, что упрощает понимание каждого куска кода;

2. Облегчается переиспользование кода - если модуль написан правильно, то есть он в себя принимает какие-то входные данные, он что-то с ними делает и отдает какие-то выходные данные;

3. Изоляция кода и его сокрытия - мы знаем, что модуль требует для своей работы, что отдает и внутри он что-то делает, но если это хорошо написанный модуль, то по сути не важно как реализованно именно его внутренний код. И удобно нам как скрывать эту функциональность и изолировать;

4. Модули позволяют нам лучше управлять зависимостями;

### Существует несколько способов написания модулей

<a id="IIFE"></a>
<details>
<summary>1. Через IIFE (Immediately Invoked Function Expression)</summary>

```js
const IIFE_module = (() => {
  let innerVariable = 'Secret value!';

  const publicInterface = {
    getValue() {
      return innerVariable;
    },
    setValue(newValue) {
      if (typeof newValue === 'string') {
        innerVariable = newValue;
      } else {
        innerVariable = "DEFAULT"
      }
    }
  }

  return publicInterface
})();

console.log(IIFE_module.getValue())

IIFE_module.setValue('new str')
console.log(IIFE_module.getValue())

IIFE_module.setValue(12334)
console.log(IIFE_module.getValue())
```
</details>

<a id="CommonJS"></a>
<details>
<summary>2. Через CommonJS</summary>

CommonJS, где импорт осуществляется через `require(moduleName)`, а экспорт - через `exports` или `module.exports`.

**Разница между `exports` и `module.exports`:**
- И `exports`, и `module.exports` изначально ссылаются на один и тот же пустой объект
- `exports` - это просто переменная-ссылка на `module.exports`
- При присваивании `exports = 'string'` мы перезаписываем эту ссылку, теряя связь с `module.exports`, поэтому модуль вернет пустой объект
- При использовании `exports.a = 'string'` мы модифицируем общий объект, поэтому свойства доступны
- `module.exports` можно безопасно перезаписывать полностью

**Работа `require()`:**
Синхронная функция `require(moduleName)` ищет модули в порядке:
1. Встроенные модули Node.js (fs, path и т.д.)
2. По относительным/абсолютным путям (`./`, `../`, `/`)
3. В папке `node_modules` текущей директории и родительских директорий

**Кеширование:** Модуль загружается только один раз при первом вызове `require()`, затем результат кешируется.
</details>

<a id="ESM"></a>
<details>
<summary>3. Через ESM (EcmaScript modules)</summary>

Выделяют следующие способы установки модуля:

a. Можно ко всем файлам вместо js установить .mjs и у нас будет использоваться import;

b. package.json написать type и выбрать "module" - так он будет понимать, что мы используем модули;

c. Через терминал написав node --input-type=module nameFile;

Есть несколько видов импортов:

- `import default from "module-name";` - дефолтный экспорт
- `import * as name from 'module-name'` - возьмется все импорты и запишется в переменную name
- `import { name_01, name_02 as Loge } from 'module-name'` - импорт с деструктиризацией и запись в переменную Loge
- `import defaultExport, { export [, [...] ] } from module-name`
- `import module-name` по типу `require('./')`;
- Динамические импорты: `import("/module-name.js").then(module => {...}).catch(error => {})`

Есть несколько видов экспорта:
- Классический экспорт `const a = 2
export { a }` и экспорт переменной через `export const a = 2`;
- Переименовка экспорта `export { a as name, b }; import { name } from 'name-file';`
- Экспорт по дефолту - когда нам необходимо экспортировать определенную функцию - `const a = 1` или просто `export default a`.
- Можно экспортировать одно по умолчанию, а другие нет - `export { a as default, b, c}`

Как работают импорты, они имеют 3 фазы:
- Construction (parsing) - в ней ищутся все импорты модулей и рекурсивно загружается контент со всех модулей
- Instantiation - для каждой сущности сохраняется именованная ссылка в памяти, но в ней пока не привязываются какие-то значения. Они помогают взаимоотношения между импортами
- Evaluation - фаза выполнения. На этой фазе node берет и выполняет код всех этих сущностей инстанцированных используя вот эти связи и после этого у нас возможен запуск кода в загружаемом модуле. Потому что все вычисления были выполнены.

Ключевые особенности:
- Native JS modules (через import)
- Импорт асинхронный
- Импорты не работают в блоках кода
- this будет undefined
- Есть import.meta
- Нет default __dirname, __filename, require, exports, module. Но заменить можно след.образом dirname:

```js
import { fileURLToPath } from 'url';
import { dirname } from 'path'; 

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename)
```

Аналог require:

```js
import {createRequire} from 'module'

const require = createRequire(import.meta.url);
```

- Ecmascript modules такие же как в браузере
</details>

---

<a id="работа-с-url"></a>
### Способы взаимодействия с путями

Существует несколько способов взаимодействия с путями, например мы можем относительно вызвать его `./files/fileToRead.txt`, но это плохая практика, так как на разных операционных систем используется разные виды слешов. Например в линуксе есть обратный слеш `\`, и это надо учитывать

```js
import path from 'path';

const fileToRead = path.join('files', 'fileToRead.txt');        // files/fileToRead.txt (Позволяет склеивать несколько участков пути)

path.resolve('src', 'app.js');                                  // '/full/path/to/current/dir/src/app.js' (создает абсолютный путь, начиная с текущей рабочей директории)

path.extname('file.txt');                                       // '.txt' (Получить расширение файла)

path.basename('/path/to/file.txt');                             // 'file.txt' (Получить имя файла с расширением)

path.basename('/path/to/file.txt', '.txt');                     // 'file' (Получить имя файла без расширения)

path.dirname('/path/to/file.txt');                              // '/path/to'  (Получить директорию)

path.normalize('/path//to/../file.txt');                        // '/path/file.txt'  (Нормализация пути (убирает лишние символы))

path.isAbsolute('/path/to/file');                               // true (Проверка абсолютный ли путь)

path.isAbsolute('./file');                                      // false (Проверка абсолютный ли путь)
```

<a id="глобальные-переменные"></a>
#### В node.js есть две глобальные переменные:

`__dirname` - строка, которая содержит путь к текущей директории. То есть на любом устройстве мы можем получить абсолютный путь от корня до дериктории, в которую мы используем эту глобальную переменную.

```js
console.log(path.join(__dirname))         // ../node-nodejs-basics/src/fs/read.js 
console.log(path.join(__dirname, ".."))   // ../node-nodejs-basics/src/fs 
```

Также стоит уточнить, что в ES-модулях не доступен dirname, вместо этого используется `import.meta.url`

```js
import { fileURLToPath } from 'url';
import { dirname } from 'path';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);
```

3. Создать new URL, у которого внутри два параметры - если мы пишем с одним параметром, то это абсолютная путь, если задействуем второй путь, то это относительный путь + базовый url

```js
const base = 'https://example.com/api/v2';

new URL('users', base);        // → https://example.com/api/v2/users
new URL('/users', base);       // → https://example.com/users
new URL('./stats', base);      // → https://example.com/api/v2/stats
new URL('../v1/users', base);  // → https://example.com/api/v1/users
```

```js
const fileToRead = new URL('./files/fileToRead.txt', import.meta.url)
// file:///C:/Users/mulwo/Desktop/pet-project/node-nodejs-basics/src/fs/read.js
```

---

<a id="module-system"></a>
### Модульная система

Node.js использует модульную систему. То есть вся встроенная функциональность разбита на отдельные пакеты или модули. Модуль представляет блок кода, который может использоваться повторно в других модулях. А так модули делятся на 3 типа: 

1\. `Packages (3-rd party)` - внешние модули, которые мы устанавливаем;

2\. `Core modules (built-in)` - это модули которые установлены по умолчанию внутри node-js, например мы импортируем их - fs, streams и.т.д.; 

3\. `Модули собственно кода` - когда мы пишем приложение на node.js мы реализуем функционал в виде какого-то кода и этот код разделяется по файлам, соответственно и эти файлы будут являться модулями

### Какие core-модули бывают

<a id="module-system-process"></a>

`Process` - это глобальный встроенный объект, который содержит информацию о текущем запущенном процессе Node.js и методы для управления им.

|Метод|Что делает| 
|----|-----| 
|`.argv` | Содержит аргументы командной строки | 
|`.env` | Содержит переменные окружения |
|`.cwd()` | В какой папке запущен процесс |
|`.cwd(directory)`| Изменяет текущию директорию, например идет на уровень выше cd .. |
|`.exit(code)` | Завершает процесс с указанным кодом (0 - успех, не 0 - ошибка) |
|`.pid` | ID текущего процесса в операционной системе |
|`.platform` | Имя платформы: 'win32', 'darwin', 'linux' и т.д. |
|`.version` | Версия ноды |

Вот наглядные [примеры](./core-modules/process/methods.js) уже в коде.

У process есть также метод для подписки на события - `process.on('event')`. Он позволяют реагировать на различные ситуации в процессе работы приложения

| События | Что делает данное событие |
| ------- | ------------------------- |
| `SIGINT` | Срабатывает при нажатии Ctrl+C в терминале (прерывание процесса) |
| `SIGTERM` | Срабатывает при получении сигнала завершения от системы (например, kill команда) |
| `warning` | Срабатывает когда Node.js генерирует предупреждение (например, об устаревшем API) |
| `exit` | Срабатывает непосредственно перед завершением процесса, но уже нельзя выполнять асинхронный код |
| `beforeExit` | Срабатывает когда очередь событий пуста, но процесс ещё не завершился. Здесь можно выполнять асинхронный код | 
| `uncaughtException` | Срабатывает когда ошибка не была перехвачена try/catch или промисом | 

Вот наглядные [примеры](./core-modules/process/event.js) уже в коде.

---

<a id="module-system-file-system"></a>

`File system` - это встроенный модуль Node.js для работы с файловой системой. Он предоставляет набор инструментов для чтения, записи, изменения и получения информации о файлах и папках на вашем компьютере. Для работы с ним, его надо импортировать - `const fs = require('fs/promises');`

| Метод | Что делает данный метод |
| ----- | ----------------------- |
| `.readFile(path, encoding)` | Читает содержимое файла |
| `.writeFile(path, data)` | Записывает данные в файл (перезаписывает) и создаст файл, если не существует.|
| `.appendFile(path, data)` | Добавляет данные в конец файла не перезаписывая файл |
| `.readdir(path)` | Читает содержимое директории |
| `.mkdir(path, options)` | Создает директорию. Необходимо использовать { recursive: true } для вложенных структур |
| `.rm(path, options)`. | Удаляет файл или директорию (рекурсивно) |
| `.rename(oldPath, newPath)` | Переименовывает или перемещает файл |
| `.copyFile(src, dest)` | Копирует файл, а также может перезаписать существующий файл без предупреждения |
| `.unlink(path)` | Удаляет файл | 
| `.realpath(path)` | Получает абсолютный путь | 
| `.access(path)` | Проверяет доступ к файлу | 
| `.stat(path)` | Статистика файла, например является ли файлом через isFile(), size - размер, birthname - дата создания файла и прочее | 

Вот наглядные [примеры](https://github.com/Mulwor/node-nodejs-basics/tree/work-with-basic-node-js/src/fs) уже в коде в RSS. Стоит отметить, что в этом репозитории тоже находятся [примеры](./core-modules/file-system/app.js')

---

<a id="module-system-os"></a>

Модуль `os` - информация об операционной системе и об аппаратном обеспечении компьютера, на котором работает Node.js. Он работает только в виде чтения. В начале нужно импортировать его `const os = require('os');`, а затем можно использовать определенные методы. Например:

| Метод | Что делает данный метод |
| ----- | ----------------------- |
| `.platform()` | Возвращает имя операционной системы (например, 'darwin', 'win32', 'linux'). |
| `.type()` |  Возвращает общее название ОС (например, 'Linux', 'Darwin', 'Windows_NT'). | 
| `.release()` | Возвращает версию (релиз) операционной системы. |
| `.version()` | Возвращает строку с версией ядра/системы (более детальная информация). |
| `.homedir()` | Возвращает путь к домашней директории текущего пользователя. |
| `.arch()` | Возвращает архитектуру процессора (например, 'x64', 'arm') | 
| `.cpus()` | Возвращает массив объектов с информацией о каждом логическом ядре процессора (модель, скорость в МГц, время работы). |
| `.totalmem()` | Возвращает общий объем оперативной памяти в байтах. | 
| `.freemem()` | Возвращает объем свободной оперативной памяти в байтах. | 
| `.networkInterfaces()` | Получаем все network интерфейсы, которые привязаны к текущему адресу | 
| `.hostname()` |  Возвращает имя хоста (компьютера) операционной системы. |

Вот [примеры кода](./core-modules/os/example.js)

---

<a id="events-and-streams"></a>

### Что такое events и streams

Стоит отметить тему `Events (события)`. Этот механизм позволяет реализовывать API для подписки на события и их генерации (эмитирования). EventEmitter - это класс, предоставляющий возможность подписываться на именованные события и генерировать их. [Пример кода](./core-modules/eventEmitter/index.js) иллюстрирует его работу.

Потоки (Streams) - это интерфейс для работы с данными по частям (чанкам), что позволяет эффективно обрабатывать большие объемы информации, не загружая их в память целиком. 

### Преимущество стримов

У стримов есть следующие преимущество:

1\. Принцип разделения ответственности. Он создает маленькие компоненты, которые делают что-то одно, но они делают это очень хорошо. Таким образом мы можем менять код изолированного чего-то одного, что позволяет лучше управлять нашим приложением

2\. Гибкость, их можно легко подключать подключать, перенаправлять (когда направляем какие-то данные из одного источника в несколько мест назначения)

3\. Эффективность по времени и памяти, так как они получают данные по кусочкам, они потребляет небольшое кол-в памяти и это потребления памяти является фиксированным потому что поток который через stream он равномерен.

### Виды и данные стримов

В node.js 4 основных видах стрима: writable (нужен для писания данных), readable (нужен для чтения данных), duplex (могут как читать так и писать), transform (вычислять на основе входных данных - выходные данные и отдавать их дальше). Стримы работают с двумя видами данных - это буфферы (или строки) и объекты. 

Буфферы - абстрактный способом хранения последовательности данных фиксированной длины. Буфферы - такие объекты, которые нужны для представления данных фиксированной длины. Это также подкласс unitInArray. Способы создания буфферов. В начале нам необходимо импортировать буфер `const { Buffer } = require('buffer')`;

| Код | Описание |
|-----|----------|
| `const emptyBuffer = Buffer.alloc(42);` | Если передаем 1 аргумент, то создается пустой буфер, который имеет размер 42 байта |
| `const filledBuffer = Buffer.alloc(42, 1);` | Если передаем 2 аргумента, то создается буфер, который имеет размер 42 байта содержащих единицу |
| `const fasterCreatedBuffer = Buffer.allocUnsafe(42, 1);` | Еще 1 способ создания буфера. Он небезопасный, работает быстрее, но может захватить больше области памяти в которых уже имеются какие-то данные и прежде чем с ним работать нужно эти данные отчистить |
| `const bufferFromStringDefaultUTF8 = Buffer.from('javascript');` | Буфер может быть создан из строки |
| `const bufferFromStringLatin1 = Buffer.from('javascript', 'latin1');` | Также вторым аргументом можем передавать кодировку |
| `const bufferFromArray = Buffer.from([1,2,3]);` | Буфер также можно создавать из массива |

[Пример кода](./streams/01-buffer.js), а также подробнее можно почитать в [документации](https://nodejs.org/api/buffer.html)

Буферы также считаются итерируемыми объектами - то есть с помощью цикла for of мы можем перебрать их. Все стримы используют какие-то внутренние буфферы для хранения каких-то данных. Кол-в данных определяется размером буффера. Чтобы узнать порог ("ограничение по памяти") можно использовать хай watermark. 

<a id="writable-stream"></a>
<h2 align="center">Writable streams</h2>

Writable streams (потоки для записи) наследуются от базового класса stream.Writable, которые находится в модуле `stream`. Они предоставляют два основных метода для работы: `writable.write()` и `writable.end().`. Примеры потоков для записи (Writable streams):

| Категория | Поток/Метод | Назначение |
|-----------|-------------|------------|
| **HTTP** | `request` (на клиентской стороне) | Отправка HTTP-запроса с клиента на сервер.
| | `response` (на серверной стороне) | Отправка HTTP-ответа с сервера клиенту. |
| **Процессы**  | [`process.stdout`](./streams/methods/process/stdout.js) | Вывод данных |
| | [`process.stderr`](./streams/methods/process/stderr.js) | Вывод ошибок |
| **Дочерние процесс** | `child.stdin` | Для передачи данных дочернему процессу (отправляет данные другой программе). |
| **Файловая система** | `fs.createWriteStream()` | Запись данных в файл. |
| **Трансформации** | `zlib` | 	Сжатие и распаковка данных (например, Gzip, Deflate). |
| | `crypto` | Шифрование и дешифрование данных |
| **Сеть** | `TCP sockets` | Передача данных по сетевым соединениям (через TCP). |

<a id="writable-api-event"></a>

Все стримы, включая writable использует API eventemitter (встроенный модуль в node.js, который предоставляет функционал нам для работы с событиями), позволяет нам эмитеть некие события, генерировать, подписаться и т.д. Основной список событий, который эмитит у нас writable stream: 

| Событие | Что делает | Когда происходит |
|-----------|-------------| -------------| 
| *.error* | Происходит при возникновении ошибки в потоке |  При любой ошибке чтения/записи или принудительном уничтожении потока с ошибкой |
| *.drain* | Сигнализирует, что можно возобновить запись данных | После того как write() возвращал false (буфер был заполнен) и буфер освободился
| *.close* | Поток закрыт и его ресурсы освобождены | После вызова writable.destroy() или при полном завершении работы потока | 
| *.finish* | Все данные успешно записаны в целевой ресурс | После вызова writable.end() и полной обработки всех буферизованных данных |

<a id="writable-полезные-свойства"></a>

### Полезные свойства

Помимо событий у него есть еще методы и какие-то полезные свойства: 

| Свойство | Что делает или для чего нужен | Пример кода | 
| -------- | ------------------------------| ----------- |
| *.write(что надо записать, callback, кодировка (по умолчанию UTF-8))* |  возвращает true/false. Если false то у нас переполнился внутренний буффер и мы должны что-то с этим сделать прежде чем писать. Например куда-то отдать эти данные | [Пример кода](./streams/writable/features/write.js) |
| *.end* | метод когда мы хотим записать какой-то последний кусочек данных, и после этого стрим у нас будет закрыт | [Пример кода](./streams/writable/features/end.js) |
| *.destroy() | делает грубое закрытие стрима (ломает). В качестве аргумента он может принимать ошибку и если мы его передали, то эту ошибку можно обработать через error | [Пример кода](./streams/writable/features/destroy.js) |
| *.cork и uncork* (редко) | Метод `.cork` - он заставляет все записанные данные буфферизоваться в памятиб он никуда никакие данные не запишет пока не будет вызван метод `.uncork`. Они работают в паре и стоит отметить, у нас кол-во `cork()` должно совпадать с `uncork()`, то есть если у одного два, то и другого должно быть столько | [Пример кода](./streams/writable/features/corkAndUnkork.js) |


Полезно использовать тогда когда chunks быстро записываются и мы не хотим их сразу быстро отправлять, мы в начале их буферизуем и нам не нужно дожидатся выполнения чанков и мы отдадим сразу все на выходе с помощью метод uncork();
 
Другие методы:
| Метод | Описание |
|-----|----------|
| writable.setDefaultEncoding(encoding) | Установить дефолтную кодировку для строк с которыми он будет работать |
| writable.destroyed | Показывает (true/false) был ли вызван метод .destroy | 
| writable.writable | Показывает можно ли еще стрим использовать для того, чтобы писать |
| writable.writableEnded | Показывает был ли вызван метод .end |
| writable.writableCorked | Показывает был ли вызван метод .cork и при этом не вызван ли метод .uncork |
| writable.writableHighWaterMark | Можем получить значения HighWaterMark для этого стрима | 
| writable.writableLength | Показывает число, байт или объектов готовые к записи |
| writable.writableNeedDrain | Флаг, который показывает нужно ли чистить наш внутренний буфер |
| writable.writableObjectMode | Геттер для object-мода, если у нас true, то стрим у нас работает в object-mode если нет, то работает со строками


<a id="readable-stream"></a>

<h2 align="center">Readable streams</h2>

Readable streams нужны нам для чтения данных. Они могут приходить постепенно (например, видео с сервера) или быть доступны сразу. К readable streams относится: 

| Категория | Поток/Метод | Назначение |
|-----------|-------------|------------|
| **HTTP** | `request` (серверная сторона) | Чтение входящих HTTP-запросов |
| | `response` (клиентская сторона) | Чтение ответов сервера |
| **Процессы** | [`process.stdin()`](./streams/methods/process/stdin.js) | Стандартный поток ввода |
| | `child process.stdout` | Стандартный вывод дочернего процесса |
| | `child process.stderr` | Поток ошибок дочернего процесса |
| **Файловая система** | `fs.createReadStream()` | Чтение файлов через потоки |
| **Трансформации** | `zlib` | Сжатие и распаковка данных |
| | `crypto` | Шифрование и дешифрование данных |
| **Сеть** | `TCP sockets` | Чтение данных из сетевых соединений |

<a id="readable-mode"></a>
<h2 align="center">Readable stream modes</h2>

1\. `Paused mode` - когда он находится в остановленном состоянии. Он не читает данные, чтобы их буфферизировать, чтобы их прочитать нам надо явно вызвать метод readable. Когда мы только создаем стрим fs.createStream - он не читает данные, он находится в paused mode, надо явно указать чтобы он начал читать 

2\. `Flowing mode` - когда он находится в текущем состоянии (в состоянии потока). Когда readable stream читает данные автоматически и отдает их дальше так быстро как это возможно и метод внутренний readable.read он вызывается автоматом

Для того, чтобы переключить с paused mode на flowing mode можно: `повесить обработчик события на data` или `Вызвать метод readable.resume (продолжить чтения)` или `Использовать readable.pipe(writable)`

А для того чтобы перевести его обратно в pause mode: `мы должны вызвать readable.pause метод, который его остановит, однако это не сработает если readable куда-то запайпили. Если у него есть какие-то пайпы, то необходимо отключить все через readable.unpipe.`

Что нужно понимать - Readable стрим не отдает данные никуда до тех пор пока не реализовали механизм, который их поглощать. 

<a id="readable-api-event"></a>
<h2 align="center">Readable api event emitter</h2>

Он тоже использует API event-emitter и имеет список событий

| Событие | Что делает | Пример кода |
|---------|------------|-------------|
| **`.data`** | При подписке на событие мы можем читать данные. Stream автоматически переходит в `Flowing mode` и нам не надо вручную дергать метод `readable.read`. | [Пример кода](./streams/readable/data.js) |
| **`.readable`** | Имитится, когда можно прочитать данные. Пользователь должен самостоятельно вызвать `readable.read()` для получения данных, у которого есть необ.параметры `([size])`, кол-во байтов для чтения. | [Пример кода](./streams/readable/readable.js) |
| **end** | Имитится, когда больше нет данных для чтения. Срабатывает только после полной обработки всех данных. | — |
| **`.pause`** | Имитится при вызове метода `readable.pause()`, если `readable.flowing` не равен `false`. Используется для приостановки потока в flowing mode. | — |
| **`.resume`** | Имитится при вызове метода `readable.resume()`, если `readable.flowing` не равен `true`. Используется для возобновления потока. | [Остановка и возобновление стрима](./streams/readable/pauseAndResume.js) |
| **`.error`** | Обработка ошибок, возникающих при работе с потоком. | — |
| **`.close`** | Срабатывает, когда стрим закрывается и все ресурсы освобождаются. | — |
|  **`.pipe`** | метод для соединения двух потоков - readable stream и writable stream |x [Пример кода](./streams/methods/pipe.js) |
| **`.unpipe`** |  метод для отсоединения двух потоков - readable stream и writable stream. Используем когда нужно переключить поток, отменить действие, когда достигает лимита | [Пример кода](./streams/methods/pipe.js) |

<details>
<summary>Код с pipe и unpipe</summary>

```js
const {createWriteStream} = require('fs');

const readableFromTerminal = process.stdin;
const writableToTerminal = process.stdout;
const writableToFile = createWriteStream('./output.txt');

readableFromTerminal.pipe(writableToTerminal)
readableFromTerminal.pipe(writableToFile)

readableFromTerminal.on('data', (chunk) => {
  const chunkStringified = chunk.toString();

  // Если наша строка не содержит unpipe terminal, то повторное написание любых слов выводится в консоль, после написать вывода никакого нету
  if (chunkStringified.match('unpipe terminal')) readableFromTerminal.unpipe(writableToTerminal)
  
  // А после него пишем это и у нас завершается выполнения кода
  if (chunkStringified.match('unpipe file')) readableFromTerminal.unpipe(writableToFile)
})
```
</details>

Что лучше выбрать readable.read в сочетании с обработкой события readable или подписку на события read - Оба варианта хороши, но лучше выбрать один стиль для всего, но предпочтительнее pipe, unpipe, a readable.read нужен для большего контроля - когда вы  хотите определять как вы хотите вызвать ваш метод read сколько вы хотите считать, что вы хотите сделать с этими данными, при использовании можно достичь большего перформанса, однако нужно знать подводные камни

Другие методы и свойства:

| Метод | Что делает | Пример кода |
|-------|------------|-------------|
| **`.destroy([error])`** | Уничтожает поток и принимает опциональный параметр `error` (объект ошибки) аналогично Writable потоку. Освобождает все ресурсы, связанные с потоком. | - |
| **`.unshift(chunk)`** | Возвращает обратно прочитанный chunk во внутренний буфер потока для последующего чтения. Полезно при парсинге данных, когда нужно "откатить" обработку. | - |
| **`.wrap(legacyStream)`** | Используется для работы с legacy-кодом. Позволяет создать современный Readable Stream на основе устаревшего потока, который будет использоваться как источник данных. | - |
| **`.isPaused()`** | Возвращает `true`, если поток остановлен (находится в paused mode), и `false` если поток активен (flowing mode). | - |
| **`[Symbol.asyncIterator]()`** | Создает асинхронный итератор для использования с `for await...of`. Автоматически предотвращает уничтожение потока при досрочном завершении цикла (break, return). Также может завершить поток при необходимости. | - |
| **`.readable`** | Флаг, показывающий, что поток находится в состоянии, пригодном для чтения данных. Возвращает `true` если поток можно читать, `false` если поток закрыт или уничтожен. | - |
| **`.readableAborted`** | Флаг, показывающий, что поток был принудительно уничтожен через метод `.destroy()` до полного чтения всех данных. | - |
| **`.readableDidRead`** | Флаг, показывающий, было ли выполнено хотя бы одно чтение данных из потока после его создания. | - |
| **`.readableEncoding`** | Возвращает кодировку, которая была установлена для потока через метод `.setEncoding()` или при создании. | - |
| **`.readableEnded`** | Возвращает `true`, если поток завершил чтение (событие `'end'` уже произошло). Показывает, что все данные были прочитаны. | - |
| **`.readableFlowing`** | Геттер, показывающий текущее состояние потока: `null` (начальное состояние, не подписан на события), `true` (flowing mode, данные поступают автоматически), `false` (paused mode, чтение приостановлено). | - |
| **`.readablePaused`** | Аналогичен `.isPaused()`. Возвращает `true`, если поток остановлен (находится в paused mode). | - |
---




<a id="duplex-streams"></a>

### Duplex streams

Это стримы, которые работают в два направления - writable interface и readable interface, то есть они могут и читать и писать. Примеры, где используются данные стримы - это `TCP sockets`, `zlib streams`, `crypto streams`

У duplex стрима есть специфичность и это `duplex.allowHalfOpen` - флаг, значения которого если будет false, то тогда когда стрим автоматически завершает свою writable часть, если завершится его readable часть. То есть разрешить ли стриму быть наполовину открытым

---

<a id="transform-streams"></a>

### Transform streams 

Это такие стримы, которые нужны для преобразования данных (разновидность duplex стримов - все интерфейсы также есть и transform). Они генерируют некий output основываясь на инпуте и это необязательно просто преобразование входных данных и отдачи в каком-то измененном виде. Например - можно отдавать больше данных чем получили, также и наоборот, либо можно вообще не отдавать данных в определнных случаях или можно то отдавать, то не отдавать. Смысл в том, что данные вычисляются выходные на основе входных при помощи какой-то внутренней реализации. Вот пример простого transform стрима, которая переварачивает строку

```js
const { Transform, pipeline } = require('stream');

const readable = process.stdin;
const writable = process.stdout;

const transform = new Transform({
    transform(chunk, eng, callback) {
        // Считываем чанк, приводим его к строке и убираем трим для убирания служеб.переводов строки и возврата корретки
        const chunkStringified = chunk.toString().trim();
        const reversedChunk = chunkStringified.split("").reverse().join('');
        this.push(reversedChunk + "\n");
        callback()
    }
})

pipeline(readable, transform, writable, err => console.log(`Error: ${err}`))
```

Есть также разновидность transform streams - это passThrough - он не преобразует данные, которые него идут, а используется для мониторинга внутри вашего пайплайна


---

<a id="полезные-фитчи-стримов"></a>

### Полезные фитчи стримов

1. `Stream.finish` - функция, которая принимает stream и получает оповещения когда stream завершается. Если это readable stream, то он больше не может читать, если writable stream, то он не может больше писать или если стрим завершился с ошибкой. Это полезно когда стрим может прерваться внезапно - например при HTTP сервера, когда мы передаем большой файл с клиента на сервер через запрос и внезапно пропадает интернет. [Пример кода находится здесь](./streams/05-stream.finished.js)

2. `Stream.pipeline` - у него есть два метода использований:
- Если просто достаем из метода stream, то это коллбек версия после всего необходимо передать коллбек для обработки ошибок
- промифицированные версия пайплайна

```js
const processData = async () => {
  try {
    // Передаем два стрима один будет читать c инпута
    // а второй будет писать с оупута 
    await pipelinePromisified(
      createReadStream('./input.txt'),
      createWriteStream('./output.txt')
    )
    console.log('Success!');
  } catch (err) {
    console.error(`Error occured: ${err}`)
  } finally {
    console.log("DO SMTH IN THE END")
  }
}
```
Чем pipeline лучше чем pipe тем, что пайплан за собой почищает вещи - корректно уничтожает все стримы, все закрывает

3. Stream.readable.from - мы можем создавать стримы из итерируемых объектов. 

```js
import { Readable } = require('stream');

const iterable = {
  from: 1,
  to: 10,
  // Для того, чтобы объект был итерируемый у него должен iterator, который
  // должен возвращать объект, который содержит метод next
  [Symbol.iterator]() {
    return {
      current: this.from,
      last: this.to,
      // Данный метод опять должен возвращать объект, в котором будет flag - done
      // завершили мы итерацию или нет и какое-то value
      next() {
        if (this.current < this.last) {
          return {
            done: false,
            value: this.curren++
          }
        }

        return {
          done: true,
          value: this.curren
        }
      }
    }
  }
}

const readableFromIterable = Readable.from(iterable);
readableFromIterable.on('data', (chunk) => {
  console.log(chunk)
})
```

### Custom streams

Мы можем создавать свои стримы с помощью прототипного наследования js, но нужны они когда мы хотим что-то создать универсальное, в 95% задач нам уже доступно все и ничего не нужно создавать. Есть следующие моменты использования своих кастомных стримов - это то что там есть `прототипное наследование`, чтобы реализовать свой стрим вы должны наследвоатся от одного из базовых классов - stream readable, writable и т.д. и убедится в том, что они вызывают конструктор родительского класса через `super()`. `Common API` вы должны придерживатся его и третий принцип - `flexibility` (гибкость)

Для того, чтобы опции, которые мы передаем в конструктор нашего кастомного стрима, чтобы они работали мы должны передавать в родительнский конструктор - вызвав super, перед тем как мы что-то запишем в this в дочернем экземпляре. Также кастомный стрим не должен вызыватся снаружи, можно использовать только публичные методы

Создавать стримы можно двумя способами:

1. Упрощенный, когда мы достаем из модуля стрим - базовый класс и создаем его экземляр. Все что он делает - это записывает в process.stdout какой-то чанк и мы его пайпим к стидн

```js
const { Writable } = require('stream');

const myWritable = new Writable({ 
  write(chunk, enc, callback) {
    process.stdout.write(chunk);
    callback();
  };
})

process.stdin.pipe(myWritable);
```

2. Гибкий - когда наследуемся от базового класса

```js
const { Writable } = require('stream');

const myWritable extends Writable { 
  constructor(options = {}) {
    super(options);
  }

  _write(chunk, enc, callback) {
    process.stdout.write(chunk);
    callback();
  };
}

const myWritable = new MyWritable();
process.stdin.pipe(myWritable);
```

Более подробно можно прочитать из документации по созданию [writable streams](https://nodejs.org/api/stream.html#implementing-a-writable-stream), [reading streams](https://nodejs.org/api/stream.html#implementing-a-readable-stream), [duplex streams](https://nodejs.org/api/stream.html#implementing-a-duplex-stream), [transform streams](https://nodejs.org/api/stream.html#implementing-a-transform-stream)


<a id="zlib"></a>
### Zlib Api - сжатия данных

Встроенный модуль для сжатия/распаковки данных с использованием алгоритмов, так как Gzip, Deflate и Brotli, что помогает уменьшить размер данных и ускорить передачу информации. Например при работе с HTTP-запросами, файлами или сетевыми соединениями, используя streams (потоки) и буферы. Приведу пару примеров, в начале нам надо импортировать `const zlib = require('zlib');`

Модуль zlib в Node.js предоставляет методы в двух формах:

- `Синхронные (Synchronous)`: Блокируют поток выполнения, подходят для простых скриптов.
- `Асинхронные на основе колбэков (Callback-based)`: Не блокируют поток, принимают функцию обратного вызова 
- `На основе потоков (Streams)`: Самый эффективный способ для работы с большими объемами данных

| Формат (алгоритм) | Синхронный метод | Асинхронный метод (callback) | Метод для потоков | 
| -------- | ---------------- | ----------------- | ----------------- |
| Gzip (сжатие) | .gzipSync() | .gzip() | .createGzip() |
| Gzip (распаковка) | .gunzipSync() | .gunzip() | .createGunzip() |
| Deflate (сжатие) | .deflateSync() | .deflate() | .createDeflate() |
| Deflate (распаковка) | .inflateSync() | .inflate() | .createInflate() |
| Brotli (сжатие) | .brotliCompressSync() | .brotliCompress() | .createBrotliCompress() |
| Brotli (распаковка) | brotliDecompressSync() | .brotliDecompress() | .createBrotliDecompress() | 

Вот примеры работы gzip:
1. [Создание и чтение GZIP-файла с помощью потоков (Streams)](./core-modules/zlib/1.js)
2. [Пример сжатия - gzip со streams - pipeline](https://github.com/Mulwor/node-nodejs-basics/blob/work-with-basic-node-js/src/zip/decompress.js)
3. [Пример распаковка со streams - pipeline](https://github.com/Mulwor/node-nodejs-basics/blob/work-with-basic-node-js/src/zip/decompress.js)
4. [Сжатие и распаковка строки (синхронно)](./core-modules/zlib/2.js) - Идеально для сжатия небольших данных, например, JSON-ответа API перед сохранением в базу.
5. [Сжатие HTTP-ответа на сервере](./core-modules/zlib/3.js) - стандартный случай использования, который экономит трафик. Клиент в запросе указывает Accept-Encoding: gzip, deflate, br, а сервер сжимает ответ, если поддерживает.

---

<a id="crypto"></a>
### Crypto API - Шифрование 

Встроенный модуля для выполнения криптографических операций: хеширование, шифрование, цифровые подписи, генерация ключей и случайных данных. Критически важен для безопасности приложений при работе с паролями, чувствительными данными, JWT-токенами, SSL/TLS. Приведу пару примеров, в начале нам надо импортировать `const crypto = require('crypto')`.

| Метод | Что делает данный метод | Code | 
| ----- | ----------------------- | ---- |
| `.createHash()` | Создает хеш-объект для генерации хешей (SHA-256, SHA-512, MD5 и др.) | [Пример]()
| `.createHmac()` | Создает HMAC (Hash-based Message Authentication Code), он проверяет целостность данных (не были ли данные изменены при передачи) и проверяет подлинность источника (действительно ли данные пришли от того, от кого должны были прийти) | [Пример](./core-modules/crypto/hmac.js) | 
| `.randomBytes()` | Генерирует криптографически безопасные случайные данные - можно использовать для паролей, ключей, токенов | [Пример](./core-modules/crypto/randomBytes()) |
| `.createCipheriv()` |  Создает объект шифратора для симметричного шифрования | [Пример](./core-modules/crypto/cipheriv.js) |
| `.createDecipheriv()` | Создает объект дешифрования (расшифрует шифр) |[Пример](./core-modules/crypto/decipheriv.js) |

Другие метод после того как закешировали => `const hash = crypto.createHash('sha256');`

| Метод | Что делает данный метод |
| ----- | ----------------------- | 
| `.update('')` | Данные для хеширования |
| `.digest('hex')` | Вычисляет хеш всех переданных через .update() данных и возвращает результат. | 
| `.copy()` | Создает копию текущего состояния хеш-объекта. Полезно для создания промежуточных хешей | 
| `.write(), .end(), .read()` | Стриминговые методы 

Вот примеры работы crypto:
1. [С использование update и digest](./core-modules/crypto/methods.js);
2. [Пример со стриминговыми методами](./core-modules/crypto/striming.js)
3. [Пример с использованием copy](./core-modules/crypto/copy.js)


<a id="module-system-http"></a>

Для сетевых запросов мы используем http - модуль который позволяет обрабатывать (как получать так и отправлять) запросы, то есть мы можем создать http сервис, 

```
const http = require('http');
const server  = http.createServer((request, response) => {
  request.end("Hello from Node.js!")
})
server.listen(4000)
```

<a id="module-system-timers-and-related"></a>

Timers & related (отложенное исполнения callback)
1. setTimeout(() => {...}, 500)
2. setInterval(() => {...}, 500)
3. setImmediate(() => {...}, 500) - выполняется всегда после пол.фазы eventloop, которые забирает новые input и output события с callback. 
4. process.nextTick(() => {...}, 500) - попадают специальные приоритетные очереди, они выполняются быстрее всего
5. queueMictotask(() => {...}, 500) - она попадает в очередь микротасок, туда же попадают и промисы, которые будут выполняться за nextTick внутри приоритетных очередей

Прочитать - https://nodejs.org/en/learn/asynchronous-work/event-loop-timers-and-nexttick

<a id="многопоточность"></a>

## Многопоточность в node.js


<a id="сluster"></a>

### Cluster

Cluster (кластер) - это встроенный модуль Node.js, который позволяет создавать дочерние процессы (workers), использующие один и тот же порт сервера. Это решает главное ограничение Node.js - однопоточность.

### Для чего он нужен? 

Node.js работает в одном потоке (основной поток event loop). Это означает:

- На многоядерных процессорах используется только одно ядро
- Длительные синхронные операции блокируют весь сервер

Cluster решает эти проблемы, создавая несколько процессов ("воркеров") для обработки запросов.

###  Основные преимущество

- Использование всех ядер CPU - каждый воркер работает на отдельном ядре
- Повышение производительности - обработка большего количества запросов параллельно
- Надежность - падение одного воркера не приводит к остановке всего сервера
- Безблокирующая работа - длительные операции в одном воркере не блокируют другие
- Простота использования - встроенный модуль, не требует дополнительных зависимостей

### Методы и свойства

Модуль cluster предоставляет различные методы для управления главным процессом (master) и воркерами (workers), а также свойство.

| Метод / свойство у cluster | Для чего нужен |
| ---------------- | -------------- | 
| .isPrimary | Возвращает true, если текущий процесс является первичным | 
| .fork() | Создает нового воркера (дочерний процесс). Вызывается только в главном процессе. |
| .setupMaster() | Настраивает параметры для создания воркеров | 
| ..disconnect([callback]) | Закрывает все воркеры и вызывает callback после завершения. |
| .workers | Объект, содержащий все активные workers (ключи - идентификаторы воркеров). | 

| Метод / свойство у worker | Для чего нужен |
| ---------------- | -------------- | 
| .id | Уникальный идентификатор воркера (доступен в объекте worker). | 
| .process | Дочерний процесс, созданный для воркера (объект ChildProcess). |
| .send() | Отправляет сообщение воркеру для обмена данными между процессами. |
| .disconnect() | Отключает воркера (завершает работу после обработки текущих соединений). |

| Событие cluster.on('event', callback) | Для чего нужен | 
| ---------------- | -------------- | 
| 'fork' | Генерируется при создании нового воркера. |
| 'online' | Генерируется, когда воркер запущен и готов к работе. |
| 'listening' | Генерируется, когда воркер начинает прослушивать порт. | 
| 'disconnect' | Генерируется при отключении воркера. | 
| 'exit' | Генерируется при завершении работы воркера. | 

Примеры использование cluster:
1. [Базовое использование (master/worker)](./cluster/1.js)
2. [Обмен сообщениями между процессами](./cluster/2.js)
3. [Управление воркерами](./cluster/3.js)
4. [Graceful shutdown](./cluster/4.js)

Эти примеры показывают:

- Как определить, в главном процессе мы или в воркере (isPrimary)
- Как создавать воркеры (fork()) и настраивать их (setupMaster())
- Как обмениваться сообщениями между процессами (send())
- Как отслеживать события жизненного цикла воркеров
- Как корректно завершать работу (disconnect())
- Как работать с объектами workers и worker

<a id="worker-threads"></a>
### Worker Threads

Модуль `worker_threads` позволяет выполнять JavaScript-код в параллельных потоках **в рамках одного процесса**, не блокируя основной поток Event Loop. Стоит отметить, что  **worker threads** не разделяют состояние приложения. Каждый воркер имеет свою изолированную копию данных,

**Основное назначение:** выполнение ресурсоемкии **CPU-интенсивных** операций (математические вычисления, обработка данных, работа с изображениями). Для простых I/O операций (чтение файлов, сетевые запросы) использование воркеров нецелесообразно - достаточно асинхронных операций.

<a id='worker-threads-key-benefit'></a>

**Ключевые преимущества:**
- Потоки работают в одном процессе (меньше накладных расходов)
- Возможность разделения памяти через `SharedArrayBuffer`
- Быстрый запуск по сравнению с созданием отдельного процесса
- Идеально подходит для параллелизации CPU-нагрузки

**Например базовый пример вычисления корня 10 млн.раз:**

**main.js:**
```javascript
const { Worker } = require('worker_threads');

const worker = new Worker('./worker.js', {
  // Передаем начальные данные: {start: 1, end: 10000000} при старте
  workerData: { start: 1, end: 10000000 }
});

worker.on('message', (result) => console.log(`Результат: ${result}`));
worker.on('error', (err) => console.error('Ошибка в воркере:', err));
worker.on('exit', (code) => {
  if (code !== 0) console.error(`Воркер завершился с кодом: ${code}`);
});
```

**worker.js:**
```javascript
// Получаем доступ к API для общения с основным потоком
const { parentPort, workerData } = require('worker_threads');

function heavyCalculation(start, end) {
  let sum = 0;
  for (let i = start; i <= end; i++) {
    // Вычисляем квадратный корень 10 миллионов раз!
    sum += Math.sqrt(i);
  }
  return sum;
}

const result = heavyCalculation(workerData.start, workerData.end);
parentPort.postMessage(result);
```

<a id="worker-threads-events"></a>

Стоит отметить, что **worker.on('')** принимает несколько событий: `online` оповещает нас, что worker запустился и выполнил начальную загрузку, `message` - получает сообщение от worker, `error` произошла ошибка в worker, `exit` - worker завершил работу. 

<a id="worker-threads-methods"></a>
Помимо события `.on` у `worker-threads` есть также свои методы и свойства. 

| Метод | Что делает данный метод | 
| ----- | ----------------------- |
| `.postMessage(value)` | Отправляет данные |
| `.terminate()`   | Асинхронно завершает работу воркера (возвращает Promise). | 
| `.ref()` - режим по умолчанию / `.unref()` | Это методы, которые контролируют должен ли Node.js ждать завершения этого воркера перед тем, как завершить работу всей программы. Когда ты создаешь воркер, Node.js по умолчанию считает его активной задачей, которую нужно дождаться. Программа не завершится, пока жив хотя бы один воркер. |
| `threadId` | Уникальный id worker |
| `.stdin, .stdout, .stderr` |	(Потоки) Доступ к стандартным потокам ввода/вывода воркера, если они были разрешены при его создании. |

Вот другие примеры использование worker threads:
1. [Вычисление суммы чисел в отдельном потоке](./worker/first/main.js)
2. [Двустороннее общение](./worker/third/main.js)
3. [Параллельная обработка массива данных](./worker/second/main.js)
4. [Пример с ref](./worker/ref/main.js)

---

<a id="childProcess"></a>

### Child process

Child process (дочерний процесс) - механизм для создания и управления дополнительными процессами вне основного потока выполнения и каждый процесс имеет свою собственную память. Это в свою очередь позволяет выполнять системные команды, запускать другие программы или скрипты параллельно. 

Основные [причина использования](./childprocess/reason.js) дочернего процесса:
1. Выполнения системных команд - 
2. Запуск внешних программ/библиотек
3. Параллельная обработка CPU-интенсивных задач. 
4. Изоляция и безопасность
- Критические ошибки в дочернем процессе не крашат основное приложение
- Возможность sandbox-режима

К недостатком можно отнести, что возможна связь только между родительским и дочерним процессом. Создания процесса - это дорогая операция по памяти

<a id="create-childProcess"></a>

Существует несколько способов создания child process
1. [exec()](./childprocess/create/exec.js) - буферизирует весь вывод команды и отдает его одной большой строкой/буфером после завершения процесса. **Используется** когда нужно выполнить короткую системную команду и получить весь результат сразу.
2. [spawn()](./childprocess/create/spawn.js) - потоковое выполнение команды с передачей данных через stdio потоки (stdin, stdout, stderr). **Используется** когда нужно обрабатывать большой объем данных или длительные процессы.
3. [fork()](./childprocess/create/fork.js) - специализированный spawn(), который создает новый экземпляр V8, что очень "тяжело" по памяти. **Используется** когда нужно запустить другой Node.js скрипт с двусторонней связью.
4. [execFile()](./childprocess/create/execFile.js) - бинарное выполнение файлов без shell. **Используется** когда нужно запустить исполняемый файл напрямую, минуя shell.

### Аргументы, который принимает каждый из методов 

1. `spawn(command, args[массив строк], options{опции})` - принимает в себя:
    - `command` - это строка, которая указывает какую программу/исполняемый файл нужно запустить. По сути отвечает за то, что где будет запускаться приложение
        - `ls` (команда в Linux/Max) - системная утилита для просмотра содержимой папки как dir в windows, 
        - `node` - сама программа node.js, которая выполняет JS-код. Пример: Когда вы пишете node server.js в командной строке - вы запускаете программу node с аргументом server.js., 
        - `git` - ну и программа для работы с гитом. 
    - `args [массив строк]` - отвечает с какими настройками/опциями ее запустить. Например: 
        - `spawn('ls', ['-la', '/home/user']);` для просмотр папки тоже самое что и в терминал ls -la /home/user
        - `spawn('node', ['server.js', '--port=3000', '--env=production']);` это то же самое что и в терминале будет => node server.js --port=3000 --env=production
        - `spawn('git', ['commit', '-m', 'initial commit']);` это тоже самое что и git commit -m "initial commit"
    - `options{опции}` - объект с дополнительными настройками процесса, которые определяют как именно будет запущен дочерний процесс. Если в первом случае мы говорили какую программу запустить, а в args с какими аргументами, то в options в каких условиях и как запустить
        - `cwd: '/home/user/projects'` — где запускать (рабочая директория).
        - `env: { NODE_ENV: 'test' }` — в каком окружении (переменные среды).
        - `stdio` — управление потоками ввода/вывода (stdin, stdout, stderr):
            - `'pipe'` (по умолчанию) — создает канал для обмена данными между процессами. Родитель может читать вывод ребенка через child.stdout.on('data', ...).
            - `'inherit'` — поток дочернего процесса подключается напрямую к соответствующему потоку родителя (например, вывод ребенка сразу идет в консоль родителя). stdio: 'inherit' применяет этот режим ко всем трем потокам.
            - `'ignore'` — поток игнорируется.
            - Можно задать массивом для каждого потока: `['ignore', 'inherit', process.stderr]` (игнорировать ввод, наследовать вывод, писать ошибки в stderr родителя).
        - `shell: true` — через что запускать. Позволяет использовать shell-синтаксис (перенаправление |, подстановки *), но запускает команду через системную оболочку (/bin/sh, cmd.exe), что менее эффективно, чем прямой вызов.
2. `exec(command, options{опции}, callback)`: 
    - `command` аналог spawn
    - `options{опции}` аналог spawn
    - `callback` - функция, вызываемая при завершении
        ```js
        (error, stdout, stderr) => {
            // error - ошибка если процесс завершился с ненулевым кодом
            // stdout - весь стандартный вывод команды (буферизированный)
            // stderr - весь вывод ошибок (буферизированный)
            if (stderr) {
              console.error(`Ошибки: ${stderr}`);
            }
        }
        ```
3. `execFile(file, args[массив строк], options{опции}, callback)`, где `file` - путь к исполняемому файлу, а `callback(error, stdout, stderr)` автоматически вызывается Node.js когда дочерний процесс завершил свою работу
4. `fork(modulePath, args[массив строк], options{опции})`, где modulePath - путь к Node.js модулю

<a id="methods-childProcess"></a>
Основные свойства и события через `const child = spawn('ls', ['-la'])`: 

| Свойства | Что делает | 
|-----|----------|
| `child.stdin` | Поток для ввода (Writable Stream) |
| `child.stdout` | Поток для вывода (Readable Stream) | 
| `child.stderr` | Поток для ошибок (Readable Stream) | 
| `child.pid` | Поток для ошибок (Readable Stream) |
| `child.killed` | Был ли процесс убит | 
| `child.exitCode` |  Код завершения | 
| `child.on('close', (code, signal) => { ... })` | Все потоки закрыты | 
| `child.on('exit', (code, signal) => { ... })` | Процесс завершен, но потоки могут быть открыты |
| `child.on('error', (err) => { ... })` | Ошибка при создании/запуске процесса | 
| `child.on('message', (message) => { ... })` | Только для fork() - сообщение через IPC | 
| `child.on('disconnect', () => { ... })` | IPC канал закрыт | 

Примеры использования:
1. [Мониторинг логов в реальном времени](./childprocess/examples/monitoring.js)
2. [Параллельная обработка файлов](./childprocess/examples/paralelFile.js)
3. [Ограничение времени выполнения](./childprocess/examples/time.js)
4. [Параллельное выполнение нескольких команд](./childprocess/examples/paralelCommand.js);
5. [Безопасное выполнение кода](./childprocess/examples/safeCommand.js);
